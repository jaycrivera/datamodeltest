{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Your file's raw GitHub URL\n",
    "url = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "\n",
    "# Fetch the file\n",
    "response = requests.get(url)\n",
    "file_data = BytesIO(response.content)\n",
    "\n",
    "# Read into pandas\n",
    "df = pd.read_excel(file_data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns in output\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Now display the dataframe\n",
    "df.head()  # or df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "url = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "response = requests.get(url)\n",
    "file_data = BytesIO(response.content)\n",
    "\n",
    "# Get list of all sheet names\n",
    "xls = pd.ExcelFile(file_data)\n",
    "print(xls.sheet_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Download the file\n",
    "url = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "response = requests.get(url)\n",
    "file_data = BytesIO(response.content)\n",
    "\n",
    "# Specify the sheet names you want\n",
    "sheets_to_load = [\n",
    "    '1. Employee Data',\n",
    "    '2. Position Perf & Pay Data',\n",
    "    '3. Demographic Data',\n",
    "    '4. Survey Data'\n",
    "]\n",
    "\n",
    "# Load them all into a dict\n",
    "dfs = pd.read_excel(file_data, sheet_name=sheets_to_load)\n",
    "\n",
    "# Now you can access each sheet like this:\n",
    "employee_df = dfs['1. Employee Data']\n",
    "position_df = dfs['2. Position Perf & Pay Data']\n",
    "demographic_df = dfs['3. Demographic Data']\n",
    "survey_df = dfs['4. Survey Data']\n",
    "\n",
    "# Example: show first 5 rows from Employee Data\n",
    "employee_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.name \"jaycrivera\"\n",
    "!git config --global user.email \"jay.rivera1994@icloud.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jaycrivera/datamodeltest.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd datamodeltest\n",
    "!git add datamodeltest.ipynb\n",
    "!git commit -m \"Backup datamodeltest.ipynb from Colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls -lah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "RAW = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "buf = BytesIO(requests.get(RAW).content)\n",
    "\n",
    "want = ['1. Employee Data','2. Position Perf & Pay Data','3. Demographic Data','4. Survey Data']\n",
    "dfs = pd.read_excel(buf, sheet_name=want)\n",
    "\n",
    "emp    = dfs['1. Employee Data']\n",
    "pos    = dfs['2. Position Perf & Pay Data']\n",
    "demo   = dfs['3. Demographic Data']\n",
    "survey = dfs['4. Survey Data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"emp_id unique?                  \", emp['emp_id'].is_unique)\n",
    "print(\"position_id unique in pos?      \", pos['position_id'].is_unique)\n",
    "print(\"All emp.position_id in pos?     \", emp['position_id'].isin(pos['position_id']).all())\n",
    "\n",
    "# Check if demo/survey IDs are actually employee IDs\n",
    "demo_is_emp   = set(demo['demographic_id']).issubset(set(emp['emp_id']))\n",
    "survey_is_emp = set(survey['survey_id']).issubset(set(emp['emp_id']))\n",
    "print(\"demographic_id ‚äÜ emp_id ?       \", demo_is_emp)\n",
    "print(\"survey_id ‚äÜ emp_id ?            \", survey_is_emp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize IDs to comparable strings\n",
    "emp_ids   = emp['emp_id'].astype(str).str.strip().str.upper()\n",
    "demo_ids  = demo['demographic_id'].astype(str).str.strip().str.upper()\n",
    "survey_ids= survey['survey_id'].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(\"demo ‚äÜ emp after normalize?\",   set(demo_ids).issubset(set(emp_ids)))\n",
    "print(\"survey ‚äÜ emp after normalize?\", set(survey_ids).issubset(set(emp_ids)))\n",
    "\n",
    "# If still false, check overlaps\n",
    "print(\"Overlap demo‚Üîemp:\",   len(set(demo_ids)   & set(emp_ids)))\n",
    "print(\"Overlap survey‚Üîemp:\", len(set(survey_ids) & set(emp_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_id ‚Üî demographic_id\n",
    "emp_id ‚Üî survey_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) CONFIG ===\n",
    "import os, datetime, subprocess, re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from getpass import getpass\n",
    "\n",
    "GITHUB_USER   = \"jaycrivera\"\n",
    "GITHUB_EMAIL  = \"jay.rivera1994@icloud.com\"\n",
    "REPO_NAME     = \"datamodeltest\"\n",
    "\n",
    "RAW_XLSX_URL  = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "SHEETS        = ['1. Employee Data','2. Position Perf & Pay Data','3. Demographic Data','4. Survey Data']\n",
    "\n",
    "OUT_DIR_LOCAL = \"outputs_tmp\"   # local temp output folder\n",
    "OUT_FILES     = {\n",
    "    \"crosswalk\": \"id_crosswalk.csv\",\n",
    "    \"merged_csv\": \"employee_view.csv\",\n",
    "    \"merged_parquet\": \"employee_view.parquet\",\n",
    "}\n",
    "\n",
    "# === 1) LOAD THE EXCEL (fresh) ===\n",
    "buf = BytesIO(requests.get(RAW_XLSX_URL).content)\n",
    "dfs = pd.read_excel(buf, sheet_name=SHEETS)\n",
    "emp    = dfs['1. Employee Data']\n",
    "pos    = dfs['2. Position Perf & Pay Data']\n",
    "demo   = dfs['3. Demographic Data']\n",
    "survey = dfs['4. Survey Data']\n",
    "\n",
    "# Basic sanity checks\n",
    "assert emp['emp_id'].is_unique, \"emp_id must be unique\"\n",
    "assert pos['position_id'].is_unique, \"position_id in Position sheet must be unique\"\n",
    "assert emp['position_id'].isin(pos['position_id']).all(), \"Some employee.position_id values missing in Position sheet\"\n",
    "assert len(emp)==len(demo)==len(survey), \"Row counts differ; index-based crosswalk impossible\"\n",
    "\n",
    "# === 2) BUILD THE CROSSWALK BY INDEX (one-time materialization) ===\n",
    "crosswalk = pd.DataFrame({\n",
    "    \"emp_id\":          emp.reset_index(drop=True)[\"emp_id\"],\n",
    "    \"demographic_id\":  demo.reset_index(drop=True)[\"demographic_id\"],\n",
    "    \"survey_id\":       survey.reset_index(drop=True)[\"survey_id\"],\n",
    "})\n",
    "\n",
    "# Optional: quick uniqueness checks\n",
    "if not crosswalk['emp_id'].is_unique:\n",
    "    raise ValueError(\"Duplicate emp_id in crosswalk (unexpected)\")\n",
    "\n",
    "# === 3) DO CLEAN MERGES USING KEYS (position_id, demographic_id, survey_id) ===\n",
    "emp_pos = emp.merge(pos, on='position_id', how='left', validate='many_to_one')\n",
    "\n",
    "# Merge demo via crosswalk key\n",
    "emp_pos = emp_pos.merge(crosswalk[['emp_id','demographic_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "full = emp_pos.merge(demo, on='demographic_id', how='left', validate='one_to_one', suffixes=(\"\",\"_demo\"))\n",
    "\n",
    "# Merge survey via crosswalk key\n",
    "full = full.merge(crosswalk[['emp_id','survey_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "full = full.merge(survey, on='survey_id', how='left', validate='one_to_one', suffixes=(\"\",\"_survey\"))\n",
    "\n",
    "# Tag provenance\n",
    "full['__join_method'] = 'index_materialized_crosswalk_v1'\n",
    "\n",
    "# === 4) SAVE OUTPUTS LOCALLY ===\n",
    "os.makedirs(OUT_DIR_LOCAL, exist_ok=True)\n",
    "crosswalk_path = os.path.join(OUT_DIR_LOCAL, OUT_FILES[\"crosswalk\"])\n",
    "merged_csv_path = os.path.join(OUT_DIR_LOCAL, OUT_FILES[\"merged_csv\"])\n",
    "merged_parquet_path = os.path.join(OUT_DIR_LOCAL, OUT_FILES[\"merged_parquet\"])\n",
    "\n",
    "crosswalk.to_csv(crosswalk_path, index=False)\n",
    "full.to_csv(merged_csv_path, index=False)\n",
    "full.to_parquet(merged_parquet_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", crosswalk_path)\n",
    "print(\" -\", merged_csv_path)\n",
    "print(\" -\", merged_parquet_path)\n",
    "\n",
    "# === 5) PUSH TO GITHUB (safe token prompt; token not printed) ===\n",
    "token = getpass(\"Paste your GitHub token (input hidden): \")\n",
    "\n",
    "# Git identity\n",
    "subprocess.run([\"git\",\"config\",\"--global\",\"user.name\", GITHUB_USER], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"--global\",\"user.email\", GITHUB_EMAIL], check=True)\n",
    "\n",
    "# Fresh clone\n",
    "subprocess.run([\"rm\",\"-rf\", REPO_NAME])\n",
    "subprocess.run([\"git\",\"clone\", f\"https://{token}@github.com/{GITHUB_USER}/{REPO_NAME}.git\"], check=True)\n",
    "\n",
    "# Create an outputs folder inside the repo (stable location)\n",
    "REPO_OUT_DIR = os.path.join(REPO_NAME, \"outputs\")\n",
    "os.makedirs(REPO_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Copy files into the repo/outputs\n",
    "subprocess.run([\"cp\", crosswalk_path, REPO_OUT_DIR + \"/\"], check=True)\n",
    "subprocess.run([\"cp\", merged_csv_path, REPO_OUT_DIR + \"/\"], check=True)\n",
    "subprocess.run([\"cp\", merged_parquet_path, REPO_OUT_DIR + \"/\"], check=True)\n",
    "\n",
    "# Commit only if there are changes\n",
    "os.chdir(REPO_NAME)\n",
    "subprocess.run([\"git\",\"add\",\"outputs\"], check=False)\n",
    "status = subprocess.run([\"git\",\"status\",\"--porcelain\"], capture_output=True, text=True)\n",
    "if status.stdout.strip():\n",
    "    msg = f\"Add crosswalk + merged employee view {datetime.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "    subprocess.run([\"git\",\"commit\",\"-m\", msg], check=True)\n",
    "    subprocess.run([\"git\",\"push\", f\"https://{token}@github.com/{GITHUB_USER}/{REPO_NAME}.git\"], check=True)\n",
    "    print(\"‚úÖ Pushed to GitHub.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No changes to commit.\")\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Cleanup\n",
    "del token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "USER   = \"jaycrivera\"\n",
    "REPO   = \"datamodeltest\"\n",
    "from getpass import getpass\n",
    "token = getpass(\"Paste your GitHub token (hidden): \")\n",
    "\n",
    "# Show current branch\n",
    "b = subprocess.run([\"git\",\"rev-parse\",\"--abbrev-ref\",\"HEAD\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "print(\"Current branch:\", b)\n",
    "\n",
    "# Set remote with username:token (works better for fine-grained PATs)\n",
    "subprocess.run([\"git\",\"remote\",\"remove\",\"origin\"], capture_output=True, text=True)\n",
    "push_url = f\"https://{USER}:{token}@github.com/{USER}/{REPO}.git\"\n",
    "r = subprocess.run([\"git\",\"remote\",\"add\",\"origin\", push_url], capture_output=True, text=True)\n",
    "if r.returncode != 0:\n",
    "    print(r.stderr)\n",
    "\n",
    "# Push to the current branch\n",
    "p = subprocess.run([\"git\",\"push\",\"-u\",\"origin\", b], capture_output=True, text=True)\n",
    "print(p.stdout)\n",
    "if p.returncode != 0:\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(p.stderr)\n",
    "    raise SystemExit(f\"git push failed with code {p.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, datetime\n",
    "from getpass import getpass\n",
    "\n",
    "NOREPLY = \"129584668+jaycrivera@users.noreply.github.com\"\n",
    "USER    = \"jaycrivera\"\n",
    "REPO    = \"datamodeltest\"\n",
    "\n",
    "# Set identity to noreply\n",
    "subprocess.run([\"git\",\"config\",\"--global\",\"user.name\", USER], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"--global\",\"user.email\", NOREPLY], check=True)\n",
    "\n",
    "# Amend last commit to use the new author email (no content changes)\n",
    "subprocess.run([\"git\",\"commit\",\"--amend\",\"--reset-author\",\"--no-edit\"], check=True)\n",
    "\n",
    "# Push again with proper auth\n",
    "token = getpass(\"Paste your GitHub token (hidden): \")\n",
    "push_url = f\"https://{USER}:{token}@github.com/{USER}/{REPO}.git\"\n",
    "subprocess.run([\"git\",\"remote\",\"remove\",\"origin\"], capture_output=True)\n",
    "subprocess.run([\"git\",\"remote\",\"add\",\"origin\", push_url], check=True)\n",
    "subprocess.run([\"git\",\"push\",\"-u\",\"origin\",\"main\"], check=True)\n",
    "print(\"‚úÖ Pushed successfully with noreply email.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === REFRESH PIPELINE: Excel -> Merge using crosswalk -> Push outputs ===\n",
    "import os, datetime, subprocess\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from getpass import getpass\n",
    "\n",
    "# ---- CONFIG ----\n",
    "USER        = \"jaycrivera\"\n",
    "REPO        = \"datamodeltest\"\n",
    "NOREPLY     = \"YOUR_NOREPLY@users.noreply.github.com\"  # <-- replace with your GitHub noreply email\n",
    "RAW_XLSX    = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "SHEETS      = ['1. Employee Data','2. Position Perf & Pay Data','3. Demographic Data','4. Survey Data']\n",
    "REPO_OUTDIR = \"outputs\"   # inside the repo\n",
    "\n",
    "# ---- AUTH (hidden) ----\n",
    "token = getpass(\"Paste your GitHub token (hidden): \")\n",
    "\n",
    "# ---- Fresh clone of repo (so we read existing crosswalk & write outputs) ----\n",
    "subprocess.run([\"rm\",\"-rf\", REPO], check=False)\n",
    "subprocess.run([\"git\",\"clone\", f\"https://{USER}:{token}@github.com/{USER}/{REPO}.git\"], check=True)\n",
    "\n",
    "# ---- Configure Git identity with noreply (avoids GH007) ----\n",
    "subprocess.run([\"git\",\"config\",\"--global\",\"user.name\", USER], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"--global\",\"user.email\", NOREPLY], check=True)\n",
    "\n",
    "# ---- Ensure outputs folder exists in repo clone ----\n",
    "os.makedirs(f\"{REPO}/{REPO_OUTDIR}\", exist_ok=True)\n",
    "\n",
    "# ---- Load latest Excel from GitHub ----\n",
    "buf = BytesIO(requests.get(RAW_XLSX).content)\n",
    "dfs = pd.read_excel(buf, sheet_name=SHEETS)\n",
    "emp, pos, demo, survey = (dfs[SHEETS[0]], dfs[SHEETS[1]], dfs[SHEETS[2]], dfs[SHEETS[3]])\n",
    "\n",
    "# ---- Basic guards ----\n",
    "assert emp['emp_id'].is_unique, \"emp_id must be unique\"\n",
    "assert pos['position_id'].is_unique, \"position_id in Position sheet must be unique\"\n",
    "assert emp['position_id'].isin(pos['position_id']).all(), \"Some employee.position_id values missing in Position sheet\"\n",
    "assert len(emp)==len(demo)==len(survey), \"Row counts differ‚Äîcannot regenerate index crosswalk safely\"\n",
    "\n",
    "# ---- Try to load the existing crosswalk; else build-by-index once ----\n",
    "crosswalk_path = f\"{REPO}/{REPO_OUTDIR}/id_crosswalk.csv\"\n",
    "if os.path.exists(crosswalk_path):\n",
    "    crosswalk = pd.read_csv(crosswalk_path, dtype=str)\n",
    "    # standardize types to string for merges\n",
    "    emp['emp_id'] = emp['emp_id'].astype(str)\n",
    "    pos['position_id'] = pos['position_id'].astype(str)\n",
    "    demo['demographic_id'] = demo['demographic_id'].astype(str)\n",
    "    survey['survey_id'] = survey['survey_id'].astype(str)\n",
    "    crosswalk[['emp_id','demographic_id','survey_id']] = crosswalk[['emp_id','demographic_id','survey_id']].astype(str)\n",
    "    join_method = \"existing_crosswalk\"\n",
    "else:\n",
    "    crosswalk = pd.DataFrame({\n",
    "        \"emp_id\":         emp.reset_index(drop=True)[\"emp_id\"].astype(str),\n",
    "        \"demographic_id\": demo.reset_index(drop=True)[\"demographic_id\"].astype(str),\n",
    "        \"survey_id\":      survey.reset_index(drop=True)[\"survey_id\"].astype(str),\n",
    "    })\n",
    "    crosswalk.to_csv(crosswalk_path, index=False)\n",
    "    join_method = \"new_index_crosswalk_v1\"\n",
    "\n",
    "# ---- Merge using keys (position_id, demographic_id, survey_id) ----\n",
    "emp_pos = emp.merge(pos, on='position_id', how='left', validate='many_to_one')\n",
    "\n",
    "emp_pos = emp_pos.merge(\n",
    "    crosswalk[['emp_id','demographic_id']], on='emp_id', how='left', validate='one_to_one'\n",
    ")\n",
    "full = emp_pos.merge(\n",
    "    demo, on='demographic_id', how='left', validate='one_to_one', suffixes=(\"\",\"_demo\")\n",
    ")\n",
    "\n",
    "full = full.merge(\n",
    "    crosswalk[['emp_id','survey_id']], on='emp_id', how='left', validate='one_to_one'\n",
    ").merge(\n",
    "    survey, on='survey_id', how='left', validate='one_to_one', suffixes=(\"\",\"_survey\")\n",
    ")\n",
    "\n",
    "full['__join_method'] = join_method\n",
    "full['__refresh_ts']  = pd.Timestamp.now(tz='UTC').isoformat()\n",
    "\n",
    "# ---- Save outputs into the repo clone ----\n",
    "merged_csv_path     = f\"{REPO}/{REPO_OUTDIR}/employee_view.csv\"\n",
    "merged_parquet_path = f\"{REPO}/{REPO_OUTDIR}/employee_view.parquet\"\n",
    "full.to_csv(merged_csv_path, index=False)\n",
    "full.to_parquet(merged_parquet_path, index=False)\n",
    "\n",
    "print(\"Saved outputs:\")\n",
    "print(\" -\", crosswalk_path, \"(exists)\" if join_method==\"existing_crosswalk\" else \"(newly created)\")\n",
    "print(\" -\", merged_csv_path)\n",
    "print(\" -\", merged_parquet_path)\n",
    "\n",
    "# ---- Commit only if there are changes; push to same branch ----\n",
    "os.chdir(REPO)\n",
    "subprocess.run([\"git\",\"add\", REPO_OUTDIR], check=False)\n",
    "status = subprocess.run([\"git\",\"status\",\"--porcelain\"], capture_output=True, text=True)\n",
    "if status.stdout.strip():\n",
    "    msg = f\"Refresh merged outputs via {join_method} on {datetime.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "    subprocess.run([\"git\",\"commit\",\"-m\", msg], check=True)\n",
    "    # Push using username:token@ (works with fine-grained tokens)\n",
    "    subprocess.run([\"git\",\"push\", f\"https://{USER}:{token}@github.com/{USER}/{REPO}.git\"], check=True)\n",
    "    print(\"‚úÖ Pushed refreshed outputs to GitHub.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No changes to commit.\")\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# ---- Best-effort cleanup ----\n",
    "del token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> add this RIGHT AFTER you load the sheets, before any merges <<<\n",
    "key_cols = {\n",
    "    \"emp\":    [\"emp_id\", \"position_id\"],\n",
    "    \"pos\":    [\"position_id\"],\n",
    "    \"demo\":   [\"demographic_id\"],\n",
    "    \"survey\": [\"survey_id\"],\n",
    "}\n",
    "\n",
    "for df, cols in [(emp, key_cols[\"emp\"]), (pos, key_cols[\"pos\"]),\n",
    "                 (demo, key_cols[\"demo\"]), (survey, key_cols[\"survey\"])]:\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# sanity\n",
    "print(emp.dtypes[\"position_id\"], pos.dtypes[\"position_id\"])  # both should show 'object'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_pos = emp.merge(pos, on='position_id', how='left', validate='many_to_one')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Row count check\n",
    "print(\"Rows in Employee Data:\", len(emp))\n",
    "print(\"Rows in merged dataframe:\", len(emp_pos))  # or `full` if you did all joins\n",
    "\n",
    "# 2) Sample columns from both sources\n",
    "print(\"Merged columns:\", emp_pos.columns.tolist()[:10], \"...\")\n",
    "\n",
    "# 3) Spot-check first few rows\n",
    "emp_pos.head()\n",
    "\n",
    "# 4) Null check on a Position column\n",
    "print(\"Missing department values:\", emp_pos['department'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final merged row count:\", len(full))\n",
    "print(\"Columns in final dataset:\", full.columns.tolist())\n",
    "print(\"Missing demographic matches:\", full['gender'].isna().sum())\n",
    "print(\"Missing survey matches:\", full['engagement_score'].isna().sum())\n",
    "\n",
    "full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mapping table\n",
    "crosswalk = pd.DataFrame({\n",
    "    \"emp_id\": emp.reset_index(drop=True)[\"emp_id\"],\n",
    "    \"demographic_id\": demo.reset_index(drop=True)[\"demographic_id\"],\n",
    "    \"survey_id\": survey.reset_index(drop=True)[\"survey_id\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Employee (emp_id) 1‚îÄ‚îÄ‚îÄ* Position (position_id)\n",
    "      ‚îÇ\n",
    "      1\n",
    "      ‚îÇ\n",
    "      *  Demographic (demographic_id)\n",
    "      ‚îÇ\n",
    "      *  Survey (survey_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Imports & config ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "RAW_XLSX_URL = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "SHEETS = [\n",
    "    '1. Employee Data',\n",
    "    '2. Position Perf & Pay Data',\n",
    "    '3. Demographic Data',\n",
    "    '4. Survey Data'\n",
    "]\n",
    "\n",
    "OUT_DIR = \"outputs_tmp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === 1) Load sheets fresh ===\n",
    "buf = BytesIO(requests.get(RAW_XLSX_URL).content)\n",
    "dfs = pd.read_excel(buf, sheet_name=SHEETS)\n",
    "\n",
    "emp    = dfs['1. Employee Data'].copy()\n",
    "pos    = dfs['2. Position Perf & Pay Data'].copy()\n",
    "demo   = dfs['3. Demographic Data'].copy()\n",
    "survey = dfs['4. Survey Data'].copy()\n",
    "\n",
    "# === 2) Normalize key dtypes (make them strings; safest) ===\n",
    "for df, cols in [\n",
    "    (emp,   ['emp_id','position_id']),\n",
    "    (pos,   ['position_id']),\n",
    "    (demo,  ['demographic_id']),\n",
    "    (survey,['survey_id'])\n",
    "]:\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# Guards for the known good join\n",
    "assert emp['emp_id'].is_unique, \"emp_id must be unique in Employee Data\"\n",
    "assert pos['position_id'].is_unique, \"position_id must be unique in Position Data\"\n",
    "assert emp['position_id'].isin(pos['position_id']).all(), \"Some employee.position_id missing in Position sheet\"\n",
    "assert len(emp) == len(demo) == len(survey), \"Counts differ; index crosswalk unsafe\"\n",
    "\n",
    "# === 3) Build/Load the crosswalk by index (test dataset constraint) ===\n",
    "crosswalk = pd.DataFrame({\n",
    "    'emp_id':         emp.reset_index(drop=True)['emp_id'],\n",
    "    'demographic_id': demo.reset_index(drop=True)['demographic_id'],\n",
    "    'survey_id':      survey.reset_index(drop=True)['survey_id'],\n",
    "})\n",
    "crosswalk_path = os.path.join(OUT_DIR, \"id_crosswalk.csv\")\n",
    "crosswalk.to_csv(crosswalk_path, index=False)\n",
    "\n",
    "# === 4) Merge all tables using real keys (position_id + crosswalk ids) ===\n",
    "emp_pos = emp.merge(pos, on='position_id', how='left', validate='many_to_one')\n",
    "\n",
    "full = (emp_pos\n",
    "        .merge(crosswalk[['emp_id','demographic_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "        .merge(demo, on='demographic_id', how='left', validate='one_to_one', suffixes=('', '_demo'))\n",
    "        .merge(crosswalk[['emp_id','survey_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "        .merge(survey, on='survey_id', how='left', validate='one_to_one', suffixes=('', '_survey'))\n",
    "       )\n",
    "\n",
    "# === 5) Derived metrics & problem flags ===\n",
    "# a) Pay benchmarks by department\n",
    "full['salary_usd'] = pd.to_numeric(full['salary_usd'], errors='coerce')\n",
    "dept_stats = full.groupby('department')['salary_usd'].agg(dept_salary_median='median').reset_index()\n",
    "full = full.merge(dept_stats, on='department', how='left')\n",
    "full['pay_gap_pct_vs_dept_median'] = (full['salary_usd'] - full['dept_salary_median']) / full['dept_salary_median']\n",
    "\n",
    "# b) Engagement/Satisfaction gaps (0 if NA)\n",
    "for col in ['engagement_score','satisfaction_score','current_performance','absences_2023','age']:\n",
    "    if col in full.columns:\n",
    "        full[col] = pd.to_numeric(full[col], errors='coerce')\n",
    "\n",
    "full['engagement_minus_satisfaction'] = full['engagement_score'] - full['satisfaction_score']\n",
    "\n",
    "# c) Problem flags (tune thresholds as needed)\n",
    "full['problem_low_engagement']   = full['engagement_score']   < 50\n",
    "full['problem_low_satisfaction'] = full['satisfaction_score'] < 50\n",
    "full['problem_high_absence']     = full['absences_2023']      > 10\n",
    "full['problem_low_perf']         = full['current_performance'] < 2\n",
    "full['problem_low_pay_vs_dept']  = full['pay_gap_pct_vs_dept_median'] < -0.15  # >15% below dept median\n",
    "\n",
    "# Optional turnover-ish flag (if present)\n",
    "if 'emp_status' in full.columns:\n",
    "    full['problem_terminated'] = full['emp_status'].str.lower().eq('terminated')\n",
    "else:\n",
    "    full['problem_terminated'] = False\n",
    "\n",
    "# === 6) Summaries: overall & by department ===\n",
    "problem_cols = [\n",
    "    'problem_low_engagement',\n",
    "    'problem_low_satisfaction',\n",
    "    'problem_high_absence',\n",
    "    'problem_low_perf',\n",
    "    'problem_low_pay_vs_dept',\n",
    "    'problem_terminated'\n",
    "]\n",
    "\n",
    "overall_counts = full[problem_cols].sum().sort_values(ascending=False)\n",
    "overall_top5 = overall_counts.head(5)\n",
    "\n",
    "by_dept = (full.groupby('department')[problem_cols]\n",
    "           .mean()\n",
    "           .mul(100)\n",
    "           .round(1)\n",
    "           .sort_values('problem_low_engagement', ascending=False))  # sort by one problem for readability\n",
    "\n",
    "print(\"=== Overall problem counts (Top 5) ===\")\n",
    "print(overall_top5)\n",
    "print(\"\\n=== Share of employees with each problem by department (%) ===\")\n",
    "print(by_dept.head(10))  # top 10 departments by low engagement rate\n",
    "\n",
    "# === 7) Save tidy outputs ===\n",
    "full_path_csv     = os.path.join(OUT_DIR, \"employee_view.csv\")\n",
    "full_path_parquet = os.path.join(OUT_DIR, \"employee_view.parquet\")\n",
    "overall_path      = os.path.join(OUT_DIR, \"problem_counts_overall.csv\")\n",
    "bydept_path       = os.path.join(OUT_DIR, \"problem_rates_by_department.csv\")\n",
    "\n",
    "full.to_csv(full_path_csv, index=False)\n",
    "full.to_parquet(full_path_parquet, index=False)\n",
    "overall_counts.to_csv(overall_path, header=['count'])\n",
    "by_dept.to_csv(bydept_path)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", crosswalk_path)\n",
    "print(\" -\", full_path_csv)\n",
    "print(\" -\", full_path_parquet)\n",
    "print(\" -\", overall_path)\n",
    "print(\" -\", bydept_path)\n",
    "\n",
    "# Peek\n",
    "full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === REFRESH PIPELINE with DRIFT GUARDS ===\n",
    "import os, json, datetime, subprocess\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from hashlib import sha256\n",
    "from getpass import getpass\n",
    "\n",
    "# ---- CONFIG ----\n",
    "USER        = \"jaycrivera\"\n",
    "REPO        = \"datamodeltest\"\n",
    "NOREPLY     = \"YOUR_NOREPLY@users.noreply.github.com\"  # <- set your GitHub noreply email\n",
    "RAW_XLSX    = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "SHEETS      = ['1. Employee Data','2. Position Perf & Pay Data','3. Demographic Data','4. Survey Data']\n",
    "REPO_OUTDIR = \"outputs\"\n",
    "CROSSWALK   = f\"{REPO_OUTDIR}/id_crosswalk.csv\"\n",
    "META        = f\"{REPO_OUTDIR}/id_crosswalk_meta.json\"\n",
    "\n",
    "def id_hash(s):\n",
    "    s = pd.Series(s, dtype=\"string\").fillna(\"\").tolist()\n",
    "    return sha256(\"|\".join(s).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# ---- AUTH (hidden) ----\n",
    "token = getpass(\"Paste your GitHub token (hidden): \")\n",
    "\n",
    "# ---- Fresh clone of repo ----\n",
    "subprocess.run([\"rm\",\"-rf\", REPO], check=False)\n",
    "subprocess.run([\"git\",\"clone\", f\"https://{USER}:{token}@github.com/{USER}/{REPO}.git\"], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"--global\",\"user.name\", USER], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"--global\",\"user.email\", NOREPLY], check=True)\n",
    "os.makedirs(f\"{REPO}/{REPO_OUTDIR}\", exist_ok=True)\n",
    "\n",
    "# ---- Load latest Excel ----\n",
    "buf = BytesIO(requests.get(RAW_XLSX).content)\n",
    "dfs = pd.read_excel(buf, sheet_name=SHEETS)\n",
    "emp, pos, demo, survey = (dfs[SHEETS[0]].copy(), dfs[SHEETS[1]].copy(), dfs[SHEETS[2]].copy(), dfs[SHEETS[3]].copy())\n",
    "\n",
    "# ---- Normalize key dtypes (strings) ----\n",
    "for df, cols in [(emp, ['emp_id','position_id']),\n",
    "                 (pos, ['position_id']),\n",
    "                 (demo, ['demographic_id']),\n",
    "                 (survey, ['survey_id'])]:\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# ---- Base guards ----\n",
    "assert emp['emp_id'].is_unique, \"emp_id must be unique in Employee Data\"\n",
    "assert pos['position_id'].is_unique, \"position_id must be unique in Position Data\"\n",
    "assert emp['position_id'].isin(pos['position_id']).all(), \"Some employee.position_id are missing in Position sheet\"\n",
    "assert len(emp) == len(demo) == len(survey), \"Counts differ; crosswalk (by index) would be unsafe\"\n",
    "\n",
    "# ---- Load or create crosswalk + DRIFT CHECKS ----\n",
    "meta_path = f\"{REPO}/{META}\"\n",
    "cw_path   = f\"{REPO}/{CROSSWALK}\"\n",
    "\n",
    "emp_ids     = emp['emp_id'].astype(str)\n",
    "demo_ids    = demo['demographic_id'].astype(str)\n",
    "survey_ids  = survey['survey_id'].astype(str)\n",
    "\n",
    "current_meta = {\n",
    "    \"emp_count\": int(len(emp_ids)),\n",
    "    \"demo_count\": int(len(demo_ids)),\n",
    "    \"survey_count\": int(len(survey_ids)),\n",
    "    \"emp_ids_hash\": id_hash(emp_ids),         # order-sensitive\n",
    "    \"demo_ids_hash\": id_hash(demo_ids),\n",
    "    \"survey_ids_hash\": id_hash(survey_ids),\n",
    "    \"generated_at\": datetime.datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "if os.path.exists(cw_path) and os.path.exists(meta_path):\n",
    "    # Validate against previous snapshot\n",
    "    prev_meta = json.load(open(meta_path))\n",
    "    problems = []\n",
    "\n",
    "    if prev_meta[\"emp_count\"] != current_meta[\"emp_count\"]:\n",
    "        problems.append(f\"emp_count changed: {prev_meta['emp_count']} -> {current_meta['emp_count']}\")\n",
    "    if prev_meta[\"demo_count\"] != current_meta[\"demo_count\"]:\n",
    "        problems.append(f\"demo_count changed: {prev_meta['demo_count']} -> {current_meta['demo_count']}\")\n",
    "    if prev_meta[\"survey_count\"] != current_meta[\"survey_count\"]:\n",
    "        problems.append(f\"survey_count changed: {prev_meta['survey_count']} -> {current_meta['survey_count']}\")\n",
    "\n",
    "    # Order-sensitive (because crosswalk depends on aligned order)\n",
    "    if prev_meta[\"emp_ids_hash\"] != current_meta[\"emp_ids_hash\"]:\n",
    "        problems.append(\"Order/content of emp_id changed (hash mismatch).\")\n",
    "    if prev_meta[\"demo_ids_hash\"] != current_meta[\"demo_ids_hash\"]:\n",
    "        problems.append(\"Order/content of demographic_id changed (hash mismatch).\")\n",
    "    if prev_meta[\"survey_ids_hash\"] != current_meta[\"survey_ids_hash\"]:\n",
    "        problems.append(\"Order/content of survey_id changed (hash mismatch).\")\n",
    "\n",
    "    if problems:\n",
    "        raise RuntimeError(\n",
    "            \"‚ùå DATA DRIFT DETECTED ‚Äî Pipeline stopped to protect crosswalk:\\n- \" + \"\\n- \".join(problems) +\n",
    "            \"\\n\\nFix: Provide an official ID mapping (emp_id ‚Üî demographic_id ‚Üî survey_id) or rebuild crosswalk explicitly after review.\"\n",
    "        )\n",
    "\n",
    "    # Crosswalk ok to reuse\n",
    "    crosswalk = pd.read_csv(cw_path, dtype=str)\n",
    "    join_method = \"existing_crosswalk\"\n",
    "else:\n",
    "    # First run: build crosswalk by index and save meta\n",
    "    crosswalk = pd.DataFrame({\n",
    "        \"emp_id\": emp_ids.reset_index(drop=True),\n",
    "        \"demographic_id\": demo_ids.reset_index(drop=True),\n",
    "        \"survey_id\": survey_ids.reset_index(drop=True)\n",
    "    })\n",
    "    crosswalk.to_csv(cw_path, index=False)\n",
    "    json.dump(current_meta, open(meta_path, \"w\"))\n",
    "    join_method = \"new_index_crosswalk_v1\"\n",
    "\n",
    "# ---- Merge using keys ----\n",
    "emp_pos = emp.merge(pos, on='position_id', how='left', validate='many_to_one')\n",
    "full = (emp_pos\n",
    "        .merge(crosswalk[['emp_id','demographic_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "        .merge(demo, on='demographic_id', how='left', validate='one_to_one', suffixes=(\"\",\"_demo\"))\n",
    "        .merge(crosswalk[['emp_id','survey_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "        .merge(survey, on='survey_id', how='left', validate='one_to_one', suffixes=(\"\",\"_survey\"))\n",
    "        .assign(__join_method=join_method,\n",
    "                __refresh_ts=pd.Timestamp.now(tz='UTC').isoformat())\n",
    "       )\n",
    "\n",
    "# ---- Derived metrics (same as before) ----\n",
    "full['salary_usd'] = pd.to_numeric(full['salary_usd'], errors='coerce')\n",
    "dept_stats = full.groupby('department')['salary_usd'].agg(dept_salary_median='median').reset_index()\n",
    "full = full.merge(dept_stats, on='department', how='left')\n",
    "full['pay_gap_pct_vs_dept_median'] = (full['salary_usd'] - full['dept_salary_median']) / full['dept_salary_median']\n",
    "\n",
    "for col in ['engagement_score','satisfaction_score','current_performance','absences_2023','age']:\n",
    "    if col in full.columns: full[col] = pd.to_numeric(full[col], errors='coerce')\n",
    "full['engagement_minus_satisfaction'] = full['engagement_score'] - full['satisfaction_score']\n",
    "\n",
    "full['problem_low_engagement']   = full['engagement_score']   < 50\n",
    "full['problem_low_satisfaction'] = full['satisfaction_score'] < 50\n",
    "full['problem_high_absence']     = full['absences_2023']      > 10\n",
    "full['problem_low_perf']         = full['current_performance'] < 2\n",
    "full['problem_low_pay_vs_dept']  = full['pay_gap_pct_vs_dept_median'] < -0.15\n",
    "full['problem_terminated']       = full.get('emp_status', pd.Series(False, index=full.index)).astype(str).str.lower().eq('terminated')\n",
    "\n",
    "# ---- Save outputs into repo clone ----\n",
    "merged_csv_path     = f\"{REPO}/{REPO_OUTDIR}/employee_view.csv\"\n",
    "merged_parquet_path = f\"{REPO}/{REPO_OUTDIR}/employee_view.parquet\"\n",
    "full.to_csv(merged_csv_path, index=False)\n",
    "full.to_parquet(merged_parquet_path, index=False)\n",
    "\n",
    "print(\"Saved outputs:\")\n",
    "print(\" -\", f\"{REPO}/{CROSSWALK}\")\n",
    "print(\" -\", f\"{REPO}/{META}\")\n",
    "print(\" -\", merged_csv_path)\n",
    "print(\" -\", merged_parquet_path)\n",
    "\n",
    "# ---- Commit/push only if changes ----\n",
    "os.chdir(REPO)\n",
    "subprocess.run([\"git\",\"add\", REPO_OUTDIR], check=False)\n",
    "status = subprocess.run([\"git\",\"status\",\"--porcelain\"], capture_output=True, text=True)\n",
    "if status.stdout.strip():\n",
    "    msg = f\"Refresh via {join_method} on {datetime.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "    subprocess.run([\"git\",\"commit\",\"-m\", msg], check=True)\n",
    "    subprocess.run([\"git\",\"push\", f\"https://{USER}:{token}@github.com/{USER}/{REPO}.git\"], check=True)\n",
    "    print(\"‚úÖ Pushed refreshed outputs to GitHub.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No changes to commit.\")\n",
    "os.chdir(\"..\")\n",
    "\n",
    "del token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, datetime\n",
    "from getpass import getpass\n",
    "\n",
    "USER    = \"jaycrivera\"\n",
    "REPO    = \"datamodeltest\"\n",
    "BRANCH  = \"main\"\n",
    "NOREPLY = \"YOUR_NOREPLY@users.noreply.github.com\"  # <- set your noreply\n",
    "\n",
    "# 0) Ensure we're in the repo root\n",
    "if os.path.basename(os.getcwd()) != REPO or not os.path.isdir(\".git\"):\n",
    "    if os.path.isdir(REPO) and os.path.isdir(os.path.join(REPO, \".git\")):\n",
    "        os.chdir(REPO)\n",
    "        print(f\"üìÇ Changed into repo: {os.getcwd()}\")\n",
    "    else:\n",
    "        raise SystemExit(\"‚ùå Run this where the cloned repo exists (e.g., /content/datamodeltest).\")\n",
    "\n",
    "# 1) Set identity (avoid GH007)\n",
    "subprocess.run([\"git\",\"config\",\"user.name\", USER], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"user.email\", NOREPLY], check=True)\n",
    "\n",
    "# 2) Auth: set remote URL with username:token\n",
    "token = getpass(\"Paste your GitHub token (hidden): \")\n",
    "push_url = f\"https://{USER}:{token}@github.com/{USER}/{REPO}.git\"\n",
    "subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", push_url], check=False)\n",
    "\n",
    "# 3) Fetch and rebase onto remote main\n",
    "print(\"‚è¨ Fetching origin...\")\n",
    "subprocess.run([\"git\",\"fetch\",\"origin\"], check=True)\n",
    "\n",
    "print(\"üîÅ Pulling with rebase...\")\n",
    "pull = subprocess.run([\"git\",\"pull\",\"--rebase\",\"origin\", BRANCH], capture_output=True, text=True)\n",
    "print(pull.stdout)\n",
    "if pull.returncode != 0:\n",
    "    print(\"---- PULL STDERR ----\")\n",
    "    print(pull.stderr)\n",
    "    # If there are conflicts, show status and stop so you can resolve\n",
    "    stat = subprocess.run([\"git\",\"status\",\"--porcelain\"], capture_output=True, text=True)\n",
    "    print(\"=== git status ===\\n\", stat.stdout)\n",
    "    raise SystemExit(\"‚ùå Rebase had conflicts. Resolve them, then run: git add <files> && git rebase --continue\")\n",
    "\n",
    "# 4) Stage your changes (outputs + .gitignore)\n",
    "subprocess.run([\"git\",\"add\",\"outputs\",\".gitignore\"], check=False)\n",
    "\n",
    "# 5) Commit if needed\n",
    "status = subprocess.run([\"git\",\"status\",\"--porcelain\"], capture_output=True, text=True)\n",
    "if status.stdout.strip():\n",
    "    msg = f\"Sync outputs after rebase on {datetime.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "    commit = subprocess.run([\"git\",\"commit\",\"-m\", msg], capture_output=True, text=True)\n",
    "    if commit.returncode != 0:\n",
    "        print(\"---- COMMIT STDERR ----\")\n",
    "        print(commit.stderr)\n",
    "        raise SystemExit(\"‚ùå git commit failed.\")\n",
    "    print(commit.stdout)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Nothing new to commit (already up to date locally).\")\n",
    "\n",
    "# 6) Push\n",
    "print(\"‚è´ Pushing to origin...\")\n",
    "push = subprocess.run([\"git\",\"push\",\"origin\", BRANCH], capture_output=True, text=True)\n",
    "print(push.stdout)\n",
    "if push.returncode != 0:\n",
    "    print(\"---- PUSH STDERR ----\")\n",
    "    print(push.stderr)\n",
    "    raise SystemExit(\"‚ùå git push failed.\")\n",
    "print(\"‚úÖ Push complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_cols = [\n",
    "    'problem_low_engagement','problem_low_satisfaction',\n",
    "    'problem_high_absence','problem_low_perf','problem_low_pay_vs_dept','problem_terminated'\n",
    "]\n",
    "full = pd.read_csv(\"outputs_tmp/employee_view.csv\")  # or load from repo if you prefer\n",
    "top5 = full[problem_cols].sum().sort_values(ascending=False).head(5)\n",
    "print(top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls -lah *.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content')\n",
    "print('Now in:', os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah /content/*.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK = \"your_uploaded_notebook.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah /content/*.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UPLOAD A NOTEBOOK + COMMIT & PUSH TO GITHUB (one-shot) ===\n",
    "import os, shutil, subprocess, datetime, urllib.parse\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from google.colab import files  # opens upload picker\n",
    "\n",
    "# --- CONFIG: edit these ---\n",
    "USER    = \"jaycrivera\"\n",
    "REPO    = \"datamodeltest\"\n",
    "BRANCH  = \"main\"\n",
    "NOREPLY = \"YOUR_NOREPLY@users.noreply.github.com\"   # put your GitHub noreply email here\n",
    "\n",
    "# 1) Upload the notebook from your computer\n",
    "print(\"üì§ Choose your .ipynb to upload...\")\n",
    "uploaded = files.upload()  # opens a browser file picker\n",
    "if not uploaded:\n",
    "    raise SystemExit(\"‚ùå No file uploaded.\")\n",
    "NOTEBOOK = next(iter(uploaded.keys()))\n",
    "nb_path = Path(\"/content\") / NOTEBOOK\n",
    "assert nb_path.exists(), f\"Upload failed: {nb_path} not found\"\n",
    "print(f\"‚úÖ Uploaded: {nb_path.name}\")\n",
    "\n",
    "# 2) Get GitHub token (hidden) and URL-encode it (handles special chars)\n",
    "token = getpass(\"Paste your GitHub token (hidden): \")\n",
    "token_enc = urllib.parse.quote(token, safe='')\n",
    "\n",
    "# 3) Fresh clone (public), then set authed remote for push\n",
    "clone_dir = Path(\"/content\") / REPO\n",
    "shutil.rmtree(clone_dir, ignore_errors=True)\n",
    "print(\"‚¨áÔ∏è Cloning repo (public)‚Ä¶\")\n",
    "subprocess.run([\"git\",\"clone\", f\"https://github.com/{USER}/{REPO}.git\"], check=True)\n",
    "\n",
    "os.chdir(clone_dir)\n",
    "print(\"üìÇ In repo:\", os.getcwd())\n",
    "\n",
    "# 4) Configure identity (avoid GH007) + remote with token\n",
    "subprocess.run([\"git\",\"config\",\"user.name\", USER], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"user.email\", NOREPLY], check=True)\n",
    "subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", f\"https://{USER}:{token_enc}@github.com/{USER}/{REPO}.git\"], check=True)\n",
    "\n",
    "# 5) Sync with remote\n",
    "pull = subprocess.run([\"git\",\"pull\",\"--rebase\",\"origin\", BRANCH], capture_output=True, text=True)\n",
    "if pull.returncode != 0:\n",
    "    print(\"---- PULL STDERR ----\\n\", pull.stderr)\n",
    "    raise SystemExit(\"‚ùå Pull (rebase) failed; check branch name or resolve conflicts.\")\n",
    "\n",
    "# 6) Copy the uploaded notebook into the repo root (overwrite if exists)\n",
    "dest = clone_dir / nb_path.name\n",
    "subprocess.run([\"cp\", str(nb_path), str(dest)], check=True)\n",
    "print(\"üì¶ Copied:\", dest.name)\n",
    "\n",
    "# 7) Commit (force empty commit if identical) and push\n",
    "subprocess.run([\"git\",\"add\", dest.name], check=True)\n",
    "msg = f\"Update {dest.name} from Colab on {datetime.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "commit = subprocess.run([\"git\",\"commit\",\"-m\", msg], capture_output=True, text=True)\n",
    "if commit.returncode != 0 and \"nothing to commit\" in (commit.stderr or \"\").lower():\n",
    "    # optional: keep history fresh even if file identical\n",
    "    subprocess.run([\"git\",\"commit\",\"--allow-empty\",\"-m\", msg + \" (empty)\"], check=True)\n",
    "elif commit.returncode != 0:\n",
    "    print(\"---- COMMIT STDERR ----\\n\", commit.stderr)\n",
    "    raise SystemExit(\"‚ùå git commit failed.\")\n",
    "\n",
    "push = subprocess.run([\"git\",\"push\",\"origin\", BRANCH], capture_output=True, text=True)\n",
    "if push.returncode != 0:\n",
    "    print(\"---- PUSH STDERR ----\\n\", push.stderr)\n",
    "    raise SystemExit(\"‚ùå git push failed. Check token scope or branch protections.\")\n",
    "print(\"‚úÖ Notebook pushed to GitHub.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# List likely locations\n",
    "!find \"/content/drive/MyDrive\" -maxdepth 3 -type f -name \"*.ipynb\" -print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PUSH NOTEBOOK FROM GOOGLE DRIVE TO GITHUB ===\n",
    "import os, shutil, subprocess, datetime, urllib.parse\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "# --- CONFIG ---\n",
    "USER, REPO, BRANCH = \"jaycrivera\", \"datamodeltest\", \"main\"\n",
    "NOREPLY = \"YOUR_NOREPLY@users.noreply.github.com\"  # your GitHub noreply email\n",
    "NOTEBOOK_ABS = \"/content/drive/MyDrive/Colab Notebooks/datamodeltestipynb.ipynb\"\n",
    "\n",
    "# Sanity check\n",
    "nb = Path(NOTEBOOK_ABS)\n",
    "assert nb.exists(), f\"Notebook not found: {nb}\"\n",
    "\n",
    "# Token\n",
    "token = getpass(\"Paste your GitHub token (hidden): \")\n",
    "token_enc = urllib.parse.quote(token, safe='')\n",
    "\n",
    "# Fresh clone\n",
    "clone_dir = Path(\"/content\")/REPO\n",
    "shutil.rmtree(clone_dir, ignore_errors=True)\n",
    "subprocess.run([\"git\",\"clone\", f\"https://github.com/{USER}/{REPO}.git\"], check=True)\n",
    "\n",
    "# Configure git\n",
    "os.chdir(clone_dir)\n",
    "subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", f\"https://{USER}:{token_enc}@github.com/{USER}/{REPO}.git\"], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"user.name\", USER], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"user.email\", NOREPLY], check=True)\n",
    "\n",
    "# Sync\n",
    "subprocess.run([\"git\",\"pull\",\"--rebase\",\"origin\", BRANCH], check=True)\n",
    "\n",
    "# Copy notebook into repo\n",
    "dest = clone_dir / nb.name\n",
    "subprocess.run([\"cp\", str(nb), str(dest)], check=True)\n",
    "\n",
    "# Commit and push\n",
    "subprocess.run([\"git\",\"add\", dest.name], check=True)\n",
    "msg = f\"Update {dest.name} from Colab on {datetime.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "commit = subprocess.run([\"git\",\"commit\",\"-m\", msg], capture_output=True, text=True)\n",
    "if commit.returncode != 0 and \"nothing to commit\" in (commit.stderr or \"\").lower():\n",
    "    subprocess.run([\"git\",\"commit\",\"--allow-empty\",\"-m\", msg + \" (empty)\"], check=True)\n",
    "elif commit.returncode != 0:\n",
    "    print(commit.stderr)\n",
    "    raise SystemExit(\"git commit failed.\")\n",
    "\n",
    "push = subprocess.run([\"git\",\"push\",\"origin\", BRANCH], capture_output=True, text=True)\n",
    "if push.returncode != 0:\n",
    "    print(push.stderr)\n",
    "    raise SystemExit(\"git push failed.\")\n",
    "\n",
    "print(\"‚úÖ Notebook pushed to GitHub.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RESET CWD -> AUTHED CLONE -> COMMIT SCRUBBED NOTEBOOK -> PUSH ===\n",
    "import os, shutil, subprocess, datetime, urllib.parse\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "# --- EDIT THESE ---\n",
    "USER, REPO, BRANCH = \"jaycrivera\", \"datamodeltest\", \"main\"\n",
    "NOREPLY = \"YOUR_NOREPLY@users.noreply.github.com\"             # <-- your GitHub noreply email\n",
    "SCRUBBED_LOCAL = \"/content/datamodeltestipynb.ipynb\"           # <-- the cleaned file we wrote earlier\n",
    "\n",
    "# 0) Make sure the scrubbed file exists\n",
    "nb = Path(SCRUBBED_LOCAL)\n",
    "assert nb.exists(), f\"Scrubbed notebook not found: {nb}\"\n",
    "\n",
    "# 1) Always start from a known-good directory\n",
    "os.chdir(\"/content\")\n",
    "Path(\"/content/work\").mkdir(exist_ok=True)\n",
    "os.chdir(\"/content/work\")\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "# 2) Get token and URL-encode it\n",
    "token = getpass(\"Paste your GitHub token (hidden): \")\n",
    "token_enc = urllib.parse.quote(token, safe='')\n",
    "\n",
    "# 3) Fresh clone into a clean folder\n",
    "clone_dir = Path(\"repo\")\n",
    "shutil.rmtree(clone_dir, ignore_errors=True)\n",
    "clone = subprocess.run(\n",
    "    [\"git\",\"clone\", f\"https://{USER}:{token_enc}@github.com/{USER}/{REPO}.git\", str(clone_dir)],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if clone.returncode != 0:\n",
    "    print(\"---- CLONE STDERR ----\")\n",
    "    print(clone.stderr)\n",
    "    raise SystemExit(\"‚ùå Clone failed. Check token scope: Repository contents ‚Üí Read & write, access to this repo.\")\n",
    "\n",
    "# 4) Configure git + sync\n",
    "os.chdir(clone_dir)\n",
    "subprocess.run([\"git\",\"config\",\"user.name\", USER], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"user.email\", NOREPLY], check=True)\n",
    "\n",
    "pull = subprocess.run([\"git\",\"pull\",\"--rebase\",\"origin\", BRANCH], capture_output=True, text=True)\n",
    "if pull.returncode != 0:\n",
    "    print(\"---- PULL STDERR ----\")\n",
    "    print(pull.stderr)\n",
    "    raise SystemExit(\"‚ùå Pull (rebase) failed. Wrong branch or branch protections?\")\n",
    "\n",
    "# 5) Copy scrubbed notebook into repo root\n",
    "dest = Path(nb.name)\n",
    "shutil.copy2(nb, dest)\n",
    "\n",
    "# 6) Commit (force empty if identical) and push\n",
    "subprocess.run([\"git\",\"add\", dest.name], check=True)\n",
    "msg = f\"Update {dest.name} (scrubbed) from Colab on {datetime.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "commit = subprocess.run([\"git\",\"commit\",\"-m\", msg], capture_output=True, text=True)\n",
    "if commit.returncode != 0:\n",
    "    if \"nothing to commit\" in (commit.stderr or \"\").lower():\n",
    "        subprocess.run([\"git\",\"commit\",\"--allow-empty\",\"-m\", msg + \" (empty)\"], check=True)\n",
    "    else:\n",
    "        print(\"---- COMMIT STDERR ----\")\n",
    "        print(commit.stderr)\n",
    "        raise SystemExit(\"‚ùå git commit failed.\")\n",
    "\n",
    "push = subprocess.run([\"git\",\"push\",\"origin\", BRANCH], capture_output=True, text=True)\n",
    "if push.returncode != 0:\n",
    "    print(\"---- PUSH STDERR ----\")\n",
    "    print(push.stderr)\n",
    "    raise SystemExit(\"‚ùå git push failed. Check token scope or branch protection rules.\")\n",
    "\n",
    "print(\"‚úÖ Clean notebook pushed to GitHub:\", f\"https://github.com/{USER}/{REPO}/blob/{BRANCH}/{dest.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build & push updated employee_view.csv to GitHub ===\n",
    "import os, shutil, subprocess, datetime, urllib.parse\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "# --- CONFIG ---\n",
    "USER, REPO, BRANCH = \"jaycrivera\", \"datamodeltest\", \"main\"\n",
    "NOREPLY = \"YOUR_NOREPLY@users.noreply.github.com\"  # <-- your GitHub noreply email\n",
    "RAW_XLSX = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "SHEETS   = ['1. Employee Data','2. Position Perf & Pay Data','3. Demographic Data','4. Survey Data']\n",
    "\n",
    "# --- 1) Clone repo (authed for push) ---\n",
    "os.chdir(\"/content\")\n",
    "shutil.rmtree(\"repo\", ignore_errors=True)\n",
    "token = getpass(\"Paste your GitHub token (hidden): \")\n",
    "tok = urllib.parse.quote(token, safe='')\n",
    "subprocess.run([\"git\",\"clone\", f\"https://{USER}:{tok}@github.com/{USER}/{REPO}.git\", \"repo\"], check=True)\n",
    "os.chdir(\"repo\")\n",
    "subprocess.run([\"git\",\"config\",\"user.name\", USER], check=True)\n",
    "subprocess.run([\"git\",\"config\",\"user.email\", NOREPLY], check=True)\n",
    "subprocess.run([\"git\",\"pull\",\"--rebase\",\"origin\", BRANCH], check=True)\n",
    "\n",
    "# --- 2) Load Excel fresh ---\n",
    "buf = BytesIO(requests.get(RAW_XLSX).content)\n",
    "dfs = pd.read_excel(buf, sheet_name=SHEETS)\n",
    "emp, pos, demo, survey = (dfs[SHEETS[0]].copy(), dfs[SHEETS[1]].copy(), dfs[SHEETS[2]].copy(), dfs[SHEETS[3]].copy())\n",
    "\n",
    "# Normalize key types\n",
    "for df, cols in [(emp, ['emp_id','position_id']),\n",
    "                 (pos, ['position_id']),\n",
    "                 (demo, ['demographic_id']),\n",
    "                 (survey, ['survey_id'])]:\n",
    "    for c in cols: df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# --- 3) Crosswalk (use existing if present; else build-by-index once) ---\n",
    "outdir = Path(\"outputs\"); outdir.mkdir(exist_ok=True, parents=True)\n",
    "cw_path = outdir / \"id_crosswalk.csv\"\n",
    "\n",
    "if cw_path.exists():\n",
    "    crosswalk = pd.read_csv(cw_path, dtype=str)\n",
    "else:\n",
    "    # WARNING: assumes current row order aligns across sheets (your test dataset case)\n",
    "    crosswalk = pd.DataFrame({\n",
    "        \"emp_id\":         emp.reset_index(drop=True)[\"emp_id\"],\n",
    "        \"demographic_id\": demo.reset_index(drop=True)[\"demographic_id\"],\n",
    "        \"survey_id\":      survey.reset_index(drop=True)[\"survey_id\"],\n",
    "    })\n",
    "    crosswalk.to_csv(cw_path, index=False)\n",
    "\n",
    "# --- 4) Merge all sheets into one tidy table ---\n",
    "emp_pos = emp.merge(pos, on='position_id', how='left', validate='many_to_one')\n",
    "\n",
    "full = (emp_pos\n",
    "        .merge(crosswalk[['emp_id','demographic_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "        .merge(demo, on='demographic_id', how='left', validate='one_to_one', suffixes=(\"\",\"_demo\"))\n",
    "        .merge(crosswalk[['emp_id','survey_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "        .merge(survey, on='survey_id', how='left', validate='one_to_one', suffixes=(\"\",\"_survey\"))\n",
    "       )\n",
    "\n",
    "# Optional derived fields useful for viz\n",
    "full['salary_usd'] = pd.to_numeric(full.get('salary_usd'), errors='coerce')\n",
    "full['engagement_score'] = pd.to_numeric(full.get('engagement_score'), errors='coerce')\n",
    "full['satisfaction_score'] = pd.to_numeric(full.get('satisfaction_score'), errors='coerce')\n",
    "full['current_performance'] = pd.to_numeric(full.get('current_performance'), errors='coerce')\n",
    "full['absences_2023'] = pd.to_numeric(full.get('absences_2023'), errors='coerce')\n",
    "\n",
    "# --- 5) Save outputs ---\n",
    "csv_path = outdir / \"employee_view.csv\"\n",
    "pq_path  = outdir / \"employee_view.parquet\"\n",
    "full.to_csv(csv_path, index=False)\n",
    "full.to_parquet(pq_path, index=False)\n",
    "\n",
    "# --- 6) Commit & push ---\n",
    "subprocess.run([\"git\",\"add\",\"outputs\"], check=True)\n",
    "msg = f\"Update employee_view.csv on {datetime.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "commit = subprocess.run([\"git\",\"commit\",\"-m\", msg], capture_output=True, text=True)\n",
    "if commit.returncode != 0 and \"nothing to commit\" in (commit.stderr or \"\").lower():\n",
    "    print(\"‚ÑπÔ∏è No changes to commit (outputs already up to date).\")\n",
    "else:\n",
    "    print(commit.stdout)\n",
    "subprocess.run([\"git\",\"push\",\"origin\", BRANCH], check=True)\n",
    "\n",
    "print(\"\\n‚úÖ Done. Download URLs:\")\n",
    "print(f\"   View: https://github.com/{USER}/{REPO}/blob/{BRANCH}/outputs/employee_view.csv\")\n",
    "print(f\"   Raw : https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/outputs/employee_view.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# point this at your local file or the raw GitHub URL\n",
    "df = pd.read_csv(\"outputs/employee_view.csv\")  # or the raw URL\n",
    "\n",
    "# quick peek at relevant-looking columns\n",
    "[c for c in df.columns if any(k in c.lower() for k in [\"status\",\"term\",\"perf\",\"engage\",\"satisf\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your merged table (adjust path if needed)\n",
    "df = pd.read_csv(\"outputs/employee_view.csv\")  # or use your raw GitHub URL\n",
    "\n",
    "# 2) Inspect columns to confirm what's actually there\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# 3) Build emp_status if missing (based on termination_year)\n",
    "if 'emp_status' not in df.columns:\n",
    "    if 'termination_year' in df.columns:\n",
    "        df['emp_status'] = df['termination_year'].notna().map({True: 'Terminated', False: 'Active'})\n",
    "    else:\n",
    "        raise ValueError(\"Can't find emp_status or termination_year. Available columns printed above.\")\n",
    "\n",
    "# 4) Find the performance column robustly\n",
    "# We‚Äôll look for common variants by case-insensitive substring\n",
    "cands = [c for c in df.columns if any(k in c.lower() for k in [\n",
    "    'current_performance', 'performance', 'perf', 'rating'\n",
    "])]\n",
    "if not cands:\n",
    "    raise ValueError(\"No performance-like column found. Tell me your column name; I‚Äôll adjust.\")\n",
    "perf_col = cands[0]\n",
    "print(\"Using performance column:\", perf_col)\n",
    "\n",
    "# 5) Create perf_bucket (map numeric to labels; clean text if already labels)\n",
    "s = df[perf_col]\n",
    "\n",
    "if pd.api.types.is_numeric_dtype(s):\n",
    "    # Heuristic mapping; tweak if your scale differs\n",
    "    perf_map = {\n",
    "        1: 'PIP',\n",
    "        2: 'Needs Improvement',\n",
    "        3: 'Fully Meets',\n",
    "        4: 'Exceeds',\n",
    "        5: 'Exceeds'  # in case it‚Äôs 1‚Äì5\n",
    "    }\n",
    "    df['perf_bucket'] = s.map(perf_map).fillna(s.astype(str))\n",
    "else:\n",
    "    # Normalize common text variants\n",
    "    t = s.astype(str).str.strip().str.lower()\n",
    "    def norm(x):\n",
    "        if any(k in x for k in ['pip', 'improvement plan']): return 'PIP'\n",
    "        if any(k in x for k in ['needs', 'below', 'under']): return 'Needs Improvement'\n",
    "        if any(k in x for k in ['fully', 'meets']):          return 'Fully Meets'\n",
    "        if any(k in x for k in ['exceed', 'outperform','outstanding','top']): return 'Exceeds'\n",
    "        return x.title()\n",
    "    df['perf_bucket'] = t.map(norm)\n",
    "\n",
    "print(\"perf_bucket uniques:\", df['perf_bucket'].dropna().unique()[:10])\n",
    "\n",
    "# 6) Crosstabs: counts + within-status %\n",
    "counts = pd.crosstab(df['emp_status'], df['perf_bucket']).sort_index()\n",
    "pct    = pd.crosstab(df['emp_status'], df['perf_bucket'], normalize='index').mul(100).round(1)\n",
    "\n",
    "print(\"\\n=== Counts ===\\n\", counts)\n",
    "print(\"\\n=== % within status ===\\n\", pct)\n",
    "\n",
    "# 7) Quick readout\n",
    "for status in counts.index:\n",
    "    top = pct.loc[status].sort_values(ascending=False).head(3)\n",
    "    print(f\"\\n{status}: \" + \", \".join([f\"{k}: {v}%\" for k,v in top.items()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "RAW = \"https://github.com/jaycrivera/datamodeltest/raw/main/Data%20Model%20Test.xlsx\"\n",
    "SHEETS = ['1. Employee Data','2. Position Perf & Pay Data','3. Demographic Data','4. Survey Data']\n",
    "\n",
    "buf = BytesIO(requests.get(RAW).content)\n",
    "dfs = pd.read_excel(buf, sheet_name=SHEETS)\n",
    "emp, pos, demo, survey = (dfs[SHEETS[0]].copy(), dfs[SHEETS[1]].copy(), dfs[SHEETS[2]].copy(), dfs[SHEETS[3]].copy())\n",
    "\n",
    "# Normalize join keys as strings\n",
    "for df, cols in [(emp,['emp_id','position_id']), (pos,['position_id']), (demo,['demographic_id']), (survey,['survey_id'])]:\n",
    "    for c in cols: df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# Build index crosswalk (test dataset assumption)\n",
    "crosswalk = pd.DataFrame({\n",
    "    \"emp_id\": emp.reset_index(drop=True)[\"emp_id\"],\n",
    "    \"demographic_id\": demo.reset_index(drop=True)[\"demographic_id\"],\n",
    "    \"survey_id\": survey.reset_index(drop=True)[\"survey_id\"],\n",
    "})\n",
    "\n",
    "# Merge WITHOUT coercing performance\n",
    "emp_pos = emp.merge(pos, on='position_id', how='left', validate='many_to_one')\n",
    "full_ok = (emp_pos\n",
    "    .merge(crosswalk[['emp_id','demographic_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "    .merge(demo, on='demographic_id', how='left', validate='one_to_one')\n",
    "    .merge(crosswalk[['emp_id','survey_id']], on='emp_id', how='left', validate='one_to_one')\n",
    "    .merge(survey, on='survey_id', how='left', validate='one_to_one')\n",
    ")\n",
    "\n",
    "# Build emp_status if needed\n",
    "if 'emp_status' not in full_ok.columns:\n",
    "    full_ok['emp_status'] = full_ok['termination_year'].notna().map({True:'Terminated', False:'Active'})\n",
    "\n",
    "# Normalize performance buckets from text or numbers\n",
    "s = full_ok['current_performance'].astype(str).str.strip().str.lower()\n",
    "def norm_perf(x):\n",
    "    if any(k in x for k in ['pip','improvement plan']): return 'PIP'\n",
    "    if any(k in x for k in ['needs','below','under']):  return 'Needs Improvement'\n",
    "    if any(k in x for k in ['fully','meets']):          return 'Fully Meets'\n",
    "    if any(k in x for k in ['exceed','outperform','top','outstanding']): return 'Exceeds'\n",
    "    # numeric fallbacks\n",
    "    if x in ('1','1.0'): return 'PIP'\n",
    "    if x in ('2','2.0'): return 'Needs Improvement'\n",
    "    if x in ('3','3.0'): return 'Fully Meets'\n",
    "    if x in ('4','4.0','5','5.0'): return 'Exceeds'\n",
    "    return x.title()\n",
    "full_ok['perf_bucket'] = s.map(norm_perf)\n",
    "\n",
    "# Crosstabs\n",
    "counts = pd.crosstab(full_ok['emp_status'], full_ok['perf_bucket'])\n",
    "pct    = pd.crosstab(full_ok['emp_status'], full_ok['perf_bucket'], normalize='index').mul(100).round(1)\n",
    "print(counts)\n",
    "print(pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load merged dataset (with clean performance data)\n",
    "df = pd.read_csv(\"outputs/employee_view.csv\")  # adjust path if needed\n",
    "\n",
    "# Ensure emp_status is present\n",
    "if 'emp_status' not in df.columns and 'termination_year' in df.columns:\n",
    "    df['emp_status'] = df['termination_year'].notna().map({True:'Terminated', False:'Active'})\n",
    "\n",
    "# Function to bucket scores\n",
    "def bucket_score(x):\n",
    "    if pd.isna(x):\n",
    "        return 'No Score'\n",
    "    elif x < 3:\n",
    "        return 'Low'\n",
    "    elif x < 4:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# Bucket engagement and satisfaction\n",
    "df['engagement_bucket']   = df['engagement_score'].apply(bucket_score)\n",
    "df['satisfaction_bucket'] = df['satisfaction_score'].apply(bucket_score)\n",
    "\n",
    "# Crosstabs\n",
    "eng_ct = pd.crosstab(df['engagement_bucket'], df['emp_status'], normalize='index').mul(100).round(1)\n",
    "sat_ct = pd.crosstab(df['satisfaction_bucket'], df['emp_status'], normalize='index').mul(100).round(1)\n",
    "\n",
    "print(\"=== Engagement bucket vs emp_status (% within bucket) ===\")\n",
    "print(eng_ct)\n",
    "print(\"\\n=== Satisfaction bucket vs emp_status (% within bucket) ===\")\n",
    "print(sat_ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load merged dataset\n",
    "df = pd.read_csv(\"outputs/employee_view.csv\")\n",
    "\n",
    "# Ensure emp_status exists\n",
    "if 'emp_status' not in df.columns and 'termination_year' in df.columns:\n",
    "    df['emp_status'] = df['termination_year'].notna().map({True:'Terminated', False:'Active'})\n",
    "\n",
    "# Drop NAs for each metric and compute group means\n",
    "eng_means = df.groupby('emp_status', dropna=True)['engagement_score'].mean().round(2)\n",
    "sat_means = df.groupby('emp_status', dropna=True)['satisfaction_score'].mean().round(2)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Average Engagement Score by Status ===\")\n",
    "print(eng_means)\n",
    "print(\"\\n=== Average Satisfaction Score by Status ===\")\n",
    "print(sat_means)\n",
    "\n",
    "# Optional: difference\n",
    "eng_diff = (eng_means['Active'] - eng_means['Terminated']).round(2)\n",
    "sat_diff = (sat_means['Active'] - sat_means['Terminated']).round(2)\n",
    "\n",
    "print(f\"\\nEngagement difference (Active - Terminated): {eng_diff}\")\n",
    "print(f\"Satisfaction difference (Active - Terminated): {sat_diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load merged dataset\n",
    "df = pd.read_csv(\"outputs/employee_view.csv\")\n",
    "\n",
    "# Create emp_status if missing\n",
    "if 'emp_status' not in df.columns and 'termination_year' in df.columns:\n",
    "    df['emp_status'] = df['termination_year'].notna().map({True:'Terminated', False:'Active'})\n",
    "\n",
    "# 1) Absences\n",
    "abs_means = df.groupby('emp_status')['absences_2023'].mean().round(2)\n",
    "abs_diff = (abs_means['Active'] - abs_means['Terminated']).round(2)\n",
    "\n",
    "# 2) Performance bucket termination rates\n",
    "if 'perf_bucket' not in df.columns:\n",
    "    # Create quick perf bucket from current_performance if missing\n",
    "    s = df['current_performance'].astype(str).str.strip().str.lower()\n",
    "    def norm_perf(x):\n",
    "        if any(k in x for k in ['pip']): return 'PIP'\n",
    "        if 'need' in x or 'below' in x: return 'Needs Improvement'\n",
    "        if 'fully' in x or 'meet' in x: return 'Fully Meets'\n",
    "        if 'exceed' in x or 'top' in x: return 'Exceeds'\n",
    "        if x in ('1','1.0'): return 'PIP'\n",
    "        if x in ('2','2.0'): return 'Needs Improvement'\n",
    "        if x in ('3','3.0'): return 'Fully Meets'\n",
    "        if x in ('4','4.0','5','5.0'): return 'Exceeds'\n",
    "        return 'No Rating'\n",
    "    df['perf_bucket'] = s.map(norm_perf)\n",
    "\n",
    "perf_ct = pd.crosstab(df['perf_bucket'], df['emp_status'], normalize='index').mul(100).round(1)\n",
    "\n",
    "# 3) Salary\n",
    "salary_means = df.groupby('emp_status')['salary_usd'].mean().round(0)\n",
    "salary_diff = (salary_means['Active'] - salary_means['Terminated']).round(0)\n",
    "\n",
    "# Output\n",
    "print(\"=== Average Absences by Status ===\")\n",
    "print(abs_means)\n",
    "print(f\"Absence difference (Active - Terminated): {abs_diff}\\n\")\n",
    "\n",
    "print(\"=== Termination % by Performance Bucket ===\")\n",
    "print(perf_ct, \"\\n\")\n",
    "\n",
    "print(\"=== Average Salary by Status ===\")\n",
    "print(salary_means)\n",
    "print(f\"Salary difference (Active - Terminated): {salary_diff}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "cleaned_at": "2025-08-08T22:48:19.185937"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
